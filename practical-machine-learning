https://www.coursera.org/learn/practical-machine-learning/home/welcome

collecting
overfitting 
80/20
accuracy
9

{adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]}
There is a non-random pattern in the plot of the outcome versus index that does not appear to be perfectly explained by any predictor suggesting a variable may be missing.
There are values of zero so when you take the log() transform those values will be -Inf., There are a large number of values that are the same and even if you took the log(SuperPlasticizer + 1) they would still all be identical so the distribution would not be symmetric.
7
0.65 0.72

PS,WS, PS, Not possible (obtained by guessing; REDO THIS QUESTION)
The bias is larger and the variance is smaller. Under leave one out cross validation K is equal to the sample size. 
2.783. It is strange because Area should be a qualitative variable - but tree is reporting the average value of Area as a numeric variable in the leaf predicted for newdata
Test Set Misclassification: 0.31 Training Set: 0.27
x.2, x.1, x.5, x.6, x.8, x.4, x.9, x.3, x.7,x.10

RF Accuracy = 0.6082 GBM Accuracy = 0.5152 Agreement Accuracy = 0.6361
Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting. 
Cement
96
6.72

1 B
2 A
3 B
4 A
5 A
6 E
7 D
8 B
9 A
10 A
11 B
12 C
13 B
14 A
15 E
16 E
17 A
18 B
19 B
20 B
